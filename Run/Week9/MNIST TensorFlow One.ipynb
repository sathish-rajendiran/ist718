{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_datasets\n",
      "  Downloading tensorflow_datasets-4.3.0-py3-none-any.whl (3.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.9 MB 4.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: attrs>=18.1.0 in /Users/sathishrajendiran/opt/anaconda3/lib/python3.8/site-packages (from tensorflow_datasets) (20.3.0)\n",
      "Collecting importlib-resources; python_version < \"3.9\"\n",
      "  Downloading importlib_resources-5.1.4-py3-none-any.whl (26 kB)\n",
      "Collecting promise\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "Requirement already satisfied: numpy in /Users/sathishrajendiran/opt/anaconda3/lib/python3.8/site-packages (from tensorflow_datasets) (1.19.2)\n",
      "Requirement already satisfied: dill in /Users/sathishrajendiran/opt/anaconda3/lib/python3.8/site-packages (from tensorflow_datasets) (0.3.3)\n",
      "Requirement already satisfied: tqdm in /Users/sathishrajendiran/opt/anaconda3/lib/python3.8/site-packages (from tensorflow_datasets) (4.50.2)\n",
      "Requirement already satisfied: future in /Users/sathishrajendiran/opt/anaconda3/lib/python3.8/site-packages (from tensorflow_datasets) (0.18.2)\n",
      "Requirement already satisfied: termcolor in /Users/sathishrajendiran/opt/anaconda3/lib/python3.8/site-packages (from tensorflow_datasets) (1.1.0)\n",
      "Collecting tensorflow-metadata\n",
      "  Downloading tensorflow_metadata-1.0.0-py3-none-any.whl (48 kB)\n",
      "\u001b[K     |████████████████████████████████| 48 kB 11.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /Users/sathishrajendiran/opt/anaconda3/lib/python3.8/site-packages (from tensorflow_datasets) (1.15.0)\n",
      "Requirement already satisfied: protobuf>=3.12.2 in /Users/sathishrajendiran/opt/anaconda3/lib/python3.8/site-packages (from tensorflow_datasets) (3.17.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/sathishrajendiran/opt/anaconda3/lib/python3.8/site-packages (from tensorflow_datasets) (2.24.0)\n",
      "Requirement already satisfied: absl-py in /Users/sathishrajendiran/opt/anaconda3/lib/python3.8/site-packages (from tensorflow_datasets) (0.12.0)\n",
      "Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in /Users/sathishrajendiran/opt/anaconda3/lib/python3.8/site-packages (from importlib-resources; python_version < \"3.9\"->tensorflow_datasets) (3.4.0)\n",
      "Collecting googleapis-common-protos<2,>=1.52.0\n",
      "  Downloading googleapis_common_protos-1.53.0-py2.py3-none-any.whl (198 kB)\n",
      "\u001b[K     |████████████████████████████████| 198 kB 27.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /Users/sathishrajendiran/opt/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->tensorflow_datasets) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/sathishrajendiran/opt/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->tensorflow_datasets) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/sathishrajendiran/opt/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->tensorflow_datasets) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/sathishrajendiran/opt/anaconda3/lib/python3.8/site-packages (from requests>=2.19.0->tensorflow_datasets) (2020.6.20)\n",
      "Building wheels for collected packages: promise\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21494 sha256=aa175df5262909d46312e5e27ed80fa03c84dec7fe9a35a4d397d66b8e8707b7\n",
      "  Stored in directory: /Users/sathishrajendiran/Library/Caches/pip/wheels/54/aa/01/724885182f93150035a2a91bce34a12877e8067a97baaf5dc8\n",
      "Successfully built promise\n",
      "Installing collected packages: importlib-resources, promise, googleapis-common-protos, tensorflow-metadata, tensorflow-datasets\n",
      "Successfully installed googleapis-common-protos-1.53.0 importlib-resources-5.1.4 promise-2.3 tensorflow-datasets-4.3.0 tensorflow-metadata-1.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SINGLE LAYER AND MULTI LAYER NETWORKS FOR MNIST\n",
    "# BASED ON CODE FROM TENSORFLOW TUTORIAL\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello, TensorFlow!'\n"
     ]
    }
   ],
   "source": [
    "# MY TEST TO MAKE SURE TF IS WORKING\n",
    "\n",
    "# hello = tf.constant('Hello, TensorFlow!')\n",
    "# sess = tf.Session()\n",
    "# print(sess.run(hello))\n",
    "\n",
    "tf.executing_eagerly()\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    hello = tf.constant('Hello, TensorFlow!')\n",
    "    print(sess.run(hello))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OBTAIN DATA\n",
    "# DATA WITHIN TF TUTORIAL HAS BEEN PREPROCESSED\n",
    "\n",
    "# from tensorflow.examples.tutorials.mnist import input_data\n",
    "# mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "\n",
    "import tensorflow_datasets\n",
    "\n",
    "\n",
    "mnist = tensorflow_datasets.load('mnist')\n",
    "\n",
    "# import input_data\n",
    "# mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "\n",
    "# from tensorflow.examples.tutorials.mnist import input_data\n",
    "# mnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test': <PrefetchDataset shapes: {image: (28, 28, 1), label: ()}, types: {image: tf.uint8, label: tf.int64}>,\n",
       " 'train': <PrefetchDataset shapes: {image: (28, 28, 1), label: ()}, types: {image: tf.uint8, label: tf.int64}>}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL\n",
    "# CREATE PLACEHOLDER VARIABLES FOR OPERATION MANIPULATION\n",
    "# THE 784 MATCHES THE VECTOR SIZE OF THE MNIST IMAGES - 28*28\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior() \n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 784])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL\n",
    "# CREATE WEIGHTS & BIASES VARIABLES\n",
    "# IN TF, OUR MODEL PARAMETERS ARE OFTEN MANAGED AS VARIABLES\n",
    "\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL\n",
    "# CREATE MODEL - USES SOFTMAX AS THE ACTIVATION FUNCTION\n",
    "# REMEMBER GOAL FOR ACTIVATION FUNCTION IS TO \"SHAPE\" THE \n",
    "# OUTPUT INTO A PROBABILITY DISTRO OVER THE 10 CLASSES\n",
    "\n",
    "y = tf.nn.softmax(tf.matmul(x, W) + b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL\n",
    "# CREATE PREDICTED VARIABLE Y-HAT\n",
    "# AND USE CROSS ENTROPY TO DETERMINE LOSS\n",
    "# CROSS ENTROPY - HOW INEFFICIENT ARE OUR PREDICTIONS?\n",
    "\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL\n",
    "# TRAIN USING GRADIENT DESCENT\n",
    "# LEARNING RATE AT MIDPOINT - .5 - MAKE SMALL ADJUSTMENTS TO MINIMIZE COST\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def next_batch(num, data, labels):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[ i] for i in idx]\n",
    "    labels_shuffle = [labels[ i] for i in idx]\n",
    "\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'PrefetchDataset' object has no attribute 'next_batch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-8a07a8dff3d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mbatch_xs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_xs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_ys\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PrefetchDataset' object has no attribute 'next_batch'"
     ]
    }
   ],
   "source": [
    "# MODEL - RUN\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "for _ in range(10000):\n",
    "  batch_xs, batch_ys = mnist['train'].next_batch(100)\n",
    "  sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.data.ops.dataset_ops.PrefetchDataset'>\n"
     ]
    }
   ],
   "source": [
    "print (mnist['train'].__class__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9203\n"
     ]
    }
   ],
   "source": [
    "# EVALUATE MODEL\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block Two for MNIST on TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code offers an alternative approach to the MNIST classification problem.\n",
    "Using two convolution layers, we can significantly improve upon the original model and explore some of the options for a deep neural network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# WEIGHT INITIALIZATION\n",
    "\n",
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CREATE CONVOLUTION AND POOLING LAYERS\n",
    "\n",
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FIRST CONVOLUTION LAYER\n",
    "\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "x_image = tf.reshape(x, [-1, 28, 28, 1]) # BASE IMAGE SIZE OF 28 * 28\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)  # RESULTING IMAGE SIZE IS 14 * 14\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SECOND CONOLUTION LAYER \n",
    "# MORE THAN ONE LAYER?  DEEP LEARNING\n",
    "\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# FULLY CONNECTED LAYER - BEFORE OUTPUT\n",
    "\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)  # ADD THE RECTIFIED LINEAR UNIT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DROP LAYER - REDUCE OVERFITTING\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LAST LAYER - OUTPUT\n",
    "\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.04\n",
      "step 100, training accuracy 0.86\n",
      "step 200, training accuracy 0.9\n",
      "step 300, training accuracy 0.9\n",
      "step 400, training accuracy 0.98\n",
      "step 500, training accuracy 0.82\n",
      "step 600, training accuracy 0.98\n",
      "step 700, training accuracy 0.96\n",
      "step 800, training accuracy 0.96\n",
      "step 900, training accuracy 0.98\n",
      "step 1000, training accuracy 0.92\n",
      "step 1100, training accuracy 0.94\n",
      "step 1200, training accuracy 0.94\n",
      "step 1300, training accuracy 0.92\n",
      "step 1400, training accuracy 1\n",
      "step 1500, training accuracy 0.92\n",
      "step 1600, training accuracy 0.98\n",
      "step 1700, training accuracy 0.94\n",
      "step 1800, training accuracy 0.96\n",
      "step 1900, training accuracy 0.98\n",
      "step 2000, training accuracy 0.98\n",
      "step 2100, training accuracy 0.98\n",
      "step 2200, training accuracy 0.98\n",
      "step 2300, training accuracy 0.96\n",
      "step 2400, training accuracy 0.96\n",
      "step 2500, training accuracy 1\n",
      "step 2600, training accuracy 0.98\n",
      "step 2700, training accuracy 1\n",
      "step 2800, training accuracy 1\n",
      "step 2900, training accuracy 0.94\n",
      "step 3000, training accuracy 1\n",
      "step 3100, training accuracy 0.98\n",
      "step 3200, training accuracy 0.98\n",
      "step 3300, training accuracy 0.96\n",
      "step 3400, training accuracy 1\n",
      "step 3500, training accuracy 0.98\n",
      "step 3600, training accuracy 0.98\n",
      "step 3700, training accuracy 1\n",
      "step 3800, training accuracy 1\n",
      "step 3900, training accuracy 0.98\n",
      "step 4000, training accuracy 0.98\n",
      "step 4100, training accuracy 1\n",
      "step 4200, training accuracy 1\n",
      "step 4300, training accuracy 0.98\n",
      "step 4400, training accuracy 1\n",
      "step 4500, training accuracy 1\n",
      "step 4600, training accuracy 1\n",
      "step 4700, training accuracy 1\n",
      "step 4800, training accuracy 1\n",
      "step 4900, training accuracy 0.98\n",
      "step 5000, training accuracy 0.98\n",
      "step 5100, training accuracy 0.98\n",
      "step 5200, training accuracy 1\n",
      "step 5300, training accuracy 0.96\n",
      "step 5400, training accuracy 0.96\n",
      "step 5500, training accuracy 0.98\n",
      "step 5600, training accuracy 1\n",
      "step 5700, training accuracy 0.98\n",
      "step 5800, training accuracy 0.98\n",
      "step 5900, training accuracy 1\n",
      "step 6000, training accuracy 1\n",
      "step 6100, training accuracy 1\n",
      "step 6200, training accuracy 1\n",
      "step 6300, training accuracy 1\n",
      "step 6400, training accuracy 0.98\n",
      "step 6500, training accuracy 1\n",
      "step 6600, training accuracy 1\n",
      "step 6700, training accuracy 0.98\n",
      "step 6800, training accuracy 0.98\n",
      "step 6900, training accuracy 1\n",
      "step 7000, training accuracy 1\n",
      "step 7100, training accuracy 1\n",
      "step 7200, training accuracy 1\n",
      "step 7300, training accuracy 1\n",
      "step 7400, training accuracy 1\n",
      "step 7500, training accuracy 0.92\n",
      "step 7600, training accuracy 1\n",
      "step 7700, training accuracy 1\n",
      "step 7800, training accuracy 1\n",
      "step 7900, training accuracy 0.98\n",
      "step 8000, training accuracy 1\n",
      "step 8100, training accuracy 1\n",
      "step 8200, training accuracy 1\n",
      "step 8300, training accuracy 1\n",
      "step 8400, training accuracy 1\n",
      "step 8500, training accuracy 1\n",
      "step 8600, training accuracy 0.98\n",
      "step 8700, training accuracy 0.98\n",
      "step 8800, training accuracy 1\n",
      "step 8900, training accuracy 1\n",
      "step 9000, training accuracy 1\n",
      "step 9100, training accuracy 1\n",
      "step 9200, training accuracy 1\n",
      "step 9300, training accuracy 1\n",
      "step 9400, training accuracy 1\n",
      "step 9500, training accuracy 1\n",
      "step 9600, training accuracy 1\n",
      "step 9700, training accuracy 1\n",
      "step 9800, training accuracy 1\n",
      "step 9900, training accuracy 1\n",
      "test accuracy 0.9906\n"
     ]
    }
   ],
   "source": [
    "# RUN THE MODEL\n",
    "\n",
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "  for i in range(10000):\n",
    "    batch = mnist.train.next_batch(50)\n",
    "    if i % 100 == 0:\n",
    "      train_accuracy = accuracy.eval(feed_dict={\n",
    "          x: batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "      print('step %d, training accuracy %g' % (i, train_accuracy))\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "\n",
    "  print('test accuracy %g' % accuracy.eval(feed_dict={\n",
    "      x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python is 3.5.3\n",
      "\n",
      "INSTALLED VERSIONS\n",
      "------------------\n",
      "commit: None\n",
      "python: 3.5.3.final.0\n",
      "python-bits: 64\n",
      "OS: Windows\n",
      "OS-release: 10\n",
      "machine: AMD64\n",
      "processor: Intel64 Family 6 Model 158 Stepping 9, GenuineIntel\n",
      "byteorder: little\n",
      "LC_ALL: None\n",
      "LANG: None\n",
      "LOCALE: None.None\n",
      "\n",
      "pandas: 0.20.1\n",
      "pytest: 3.0.7\n",
      "pip: 9.0.1\n",
      "setuptools: 27.2.0\n",
      "Cython: 0.25.2\n",
      "numpy: 1.12.1\n",
      "scipy: 0.19.1\n",
      "xarray: None\n",
      "IPython: 5.3.0\n",
      "sphinx: 1.5.6\n",
      "patsy: 0.4.1\n",
      "dateutil: 2.6.0\n",
      "pytz: 2017.2\n",
      "blosc: None\n",
      "bottleneck: 1.2.1\n",
      "tables: 3.2.2\n",
      "numexpr: 2.6.2\n",
      "feather: None\n",
      "matplotlib: 2.0.2\n",
      "openpyxl: 2.4.7\n",
      "xlrd: 1.0.0\n",
      "xlwt: 1.2.0\n",
      "xlsxwriter: 0.9.6\n",
      "lxml: 3.7.3\n",
      "bs4: 4.6.0\n",
      "html5lib: 0.9999999\n",
      "sqlalchemy: 1.1.9\n",
      "pymysql: None\n",
      "psycopg2: None\n",
      "jinja2: 2.9.6\n",
      "s3fs: None\n",
      "pandas_gbq: None\n",
      "pandas_datareader: None\n"
     ]
    }
   ],
   "source": [
    "# System Information\n",
    "import platform\n",
    "import pandas as pd\n",
    "print('Python is ' + platform.python_version())\n",
    "\n",
    "pd.show_versions(as_json=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
